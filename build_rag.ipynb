{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac59efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ac197",
   "metadata": {},
   "source": [
    "#### build index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd94e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "text_spliter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=500, chunk_overlap=100\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8537e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata: {'Header 1': 'ONE ZERO Bank Guide on Card Usage and Services', 'Header 2': 'Using the mobile application', 'Header 3': 'Cards via the mobile application - issue a new card, card-related actions, PIN code, join digital wallet', 'source': 'cards.md'}\n",
      "Content: You can order a new card, cancel a card, replace a card, view your card transactions, view or change...\n",
      "--------------------\n",
      "Metadata: {'Header 1': 'ONE ZERO Bank Guide on Card Usage and Services', 'Header 2': 'Using the mobile application', 'Header 3': 'Viewing card transactions and card fees', 'source': 'cards.md'}\n",
      "Content: You can view the transactions and fees on your card in the credit card company - [Isracard](https://...\n",
      "--------------------\n",
      "Metadata: {'Header 1': 'ONE ZERO Bank Guide on Card Usage and Services', 'Header 2': 'Traveling Abroad', 'Header 3': 'What do you need to know before traveling abroad?', 'source': 'cards.md'}\n",
      "Content: A few important things you should know before traveling:  \n",
      "* You should update the credit card compa...\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_dir = 'docs'\n",
    "doc_splits = []\n",
    "\n",
    "for path in glob.glob(os.path.join(docs_dir, '*')):\n",
    "    if path.lower().endswith(('.md', '.txt')):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "            \n",
    "            # Use split_text, not split_documents\n",
    "            header_splits = markdown_splitter.split_text(file_content)\n",
    "            \n",
    "            # Optional: Add the filename to metadata for each split\n",
    "            for chunk in header_splits:\n",
    "                chunk.metadata[\"source\"] = os.path.basename(path)\n",
    "            \n",
    "            doc_splits.extend(header_splits)\n",
    "\n",
    "# # Show the result\n",
    "# for chunk in doc_splits[:3]: # Look at first 3 chunks\n",
    "#     print(f\"Metadata: {chunk.metadata}\")\n",
    "#     print(f\"Content: {chunk.page_content[:100]}...\")\n",
    "#     print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ad3aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "        documents=doc_splits, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0d12ae89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4d0624a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store_md_spliter(docs_dir='docs',save_path =\"vector_store_data_md_spliter.json\",text_splitter=text_spliter, embeddings=embeddings):\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "    docs_dir = 'docs'\n",
    "    doc_splits = []\n",
    "\n",
    "    for path in glob.glob(os.path.join(docs_dir, '*')):\n",
    "        if path.lower().endswith(('.md', '.txt')):\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                file_content = f.read()\n",
    "                \n",
    "                # Use split_text, not split_documents\n",
    "                header_splits = markdown_splitter.split_text(file_content)\n",
    "                \n",
    "                # Optional: Add the filename to metadata for each split\n",
    "                for chunk in header_splits:\n",
    "                    chunk.metadata[\"source\"] = os.path.basename(path)\n",
    "                \n",
    "                doc_splits.extend(header_splits)\n",
    "    \n",
    "\n",
    "    vector_store = InMemoryVectorStore.from_documents(\n",
    "        documents=doc_splits, embedding=embeddings)\n",
    "\n",
    "    # vector_store.save(save_path)\n",
    "    vector_store.dump(save_path)\n",
    "    return vector_store\n",
    "\n",
    "def create_vector_store(docs_dir='docs',save_path =\"vector_store_data.json\",text_splitter=text_spliter, embeddings=embeddings):\n",
    "    import glob\n",
    "    from langchain.schema import Document\n",
    "    docs = []\n",
    "    docs_dir = 'docs'\n",
    "    for path in glob.glob(os.path.join(docs_dir, '*')):\n",
    "        # only load markdown and text files (adjust as needed)\n",
    "        if path.lower().endswith(('.md', '.txt')):\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            docs.append([Document(page_content=text, metadata={'source': os.path.basename(path)})])\n",
    "    \n",
    "\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "    \n",
    "\n",
    "    vector_store = InMemoryVectorStore.from_documents(\n",
    "        documents=doc_splits, embedding=embeddings)\n",
    "\n",
    "    # vector_store.save(save_path)\n",
    "    vector_store.dump(save_path)\n",
    "    return vector_store\n",
    "\n",
    "def retrieve_context(query: str,vector_store,k=5):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=k)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fc93140",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_vector_store_md_spliter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5bf97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrived = retrieved_docs = vector_store.similarity_search(\"What are the fees for using my card abroad if I am on the ZERO subscription plan?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9df56487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8484ac25-4f0a-4eff-8e98-51911f216333'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrived[0].id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8161a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vector_store = InMemoryVectorStore.load(\n",
    "     \"vector_store_data_md_spliter.json\", \n",
    "     embedding=embeddings # You must pass the same embedding object used to create it\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b165a774",
   "metadata": {},
   "source": [
    "### create the QNA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17114bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from urllib import response\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"gpt-4.1\")\n",
    "\n",
    "def answer_query(query: str, vector_store, model):\n",
    "    context = retrieve_context(query, vector_store)\n",
    "    results = model.invoke([\n",
    "    {\"role\": \"system\", \"content\": \"Use the following context to answer the question:{query}. If the context does not contain the answer, say you don't know. if you use spesific information from the context, tell the user the source headers.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\n Question: {query}? /n/n your answer: /n/n\"}\n",
    "    ]).content\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "88b9b66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To add your credit card to your phone\\'s digital wallet, follow these steps based on your device type:\\n\\nFor Apple Devices (iPhone, iPad):\\n(Source: Setting Up Apple Pay on Apple Devices, cards.md)\\n\\n- Open your device settings.\\n- Tap on your name, then go to \"Payment and Shipping.\"\\n- Select \"Add Payment Method\" and enter your credit card details.\\n- Alternatively, you can add your card directly through the Cards page in your ONE ZERO Bank app: Go to the Cards page → Add to Wallet → select your device.\\n- You can also follow this [step-by-step Apple Pay video](https://www.youtube.com/watch?v=nrBfsXHZO8s).\\n- If you want to change the default card: Go to iPhone settings → Wallet & Apple Pay → select the desired card under \"Transaction Defaults.\"\\n\\nApple allows up to 8 charge cards to be linked, and all must belong to the same owner.\\n\\nFor Android Devices (Google Pay):\\n(Source: Setting Up Google Pay on Android Devices, cards.md)\\n\\n- Download the Google Pay app from the Play Store.\\n- Open the app and enter your credit card details (16-digit number, expiration date, CVV).\\n- Fill in your personal information, accept the terms, and enter the code you receive by SMS.\\n- Alternatively, you can use the Isracard app: Open the app, select \"Sign up to Google Pay,\" choose your card, and click Add to Google Pay.\\n- The first card you add will be set as default, but you can change it in the Google Pay app.\\n\\nGoogle does not limit the number of cards you can add.\\n\\nIf you experience any technical issues or your card does not work after following these steps, you can contact a private banker for assistance.\\n\\nSources:\\n- Setting Up Apple Pay on Apple Devices (cards.md)\\n- Setting Up Google Pay on Android Devices (cards.md)\\n- Connecting to the digital wallet (cards.md)'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query(query=\"How can I add my credit card to my phone's digital wallet?\", vector_store=vector_store,model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab1ad863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The purpose of this project is to provide ONE ZERO Bank clients with comprehensive guidance, tools, and informational resources related to securities trading and portfolio management. The project aims to streamline the investment process by offering:\\n\\n- Educational materials about various investment instruments (such as tracking funds, ETFs, mutual funds, stocks, and bonds).\\n- AI-generated reviews and updates on clients’ stock portfolios.\\n- Objective news feeds on stocks, gathered via third-party AI-driven methods.\\n- Insights into market mechanics, regulatory considerations, risk management, and trading instructions.\\n- Access to personalized banking services, budgeting advice, and long-term financial planning.\\n\\nThe overarching goal is to empower investors with knowledge and digital tools to make informed decisions, manage risks, and efficiently achieve their financial and investment objectives—while clarifying that all information provided is for informational purposes only and does not constitute personalized investment advice.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query(\"What is the purpose of this project?\", vector_store, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ffadcd",
   "metadata": {},
   "source": [
    "#### todo - rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708fa1d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flashrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflashrank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ranker, RerankRequest\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_context_with_rerank\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, vector_store, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Retrieves documents and reranks them using FlashRank directly (bypassing LangChain wrapper).\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flashrank'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_compressors.flashrank_rerank import FlashrankRerank\n",
    "\n",
    "def retrieve_context_with_rerank(query: str, vector_store, k=5, fetch_k=20):\n",
    "    \"\"\"\n",
    "    Retrieves documents and then reranks them using a local Cross-Encoder.\n",
    "    \n",
    "    Args:\n",
    "        query: The user query.\n",
    "        vector_store: The vector store instance.\n",
    "        k: The final number of documents to return to the LLM.\n",
    "        fetch_k: The initial number of documents to fetch from the vector store (usually 3x-4x of k).\n",
    "    \"\"\"\n",
    "    # 1. Initial Retrieval: Get a larger candidate set (fetch_k)\n",
    "    # We fetch more docs because vector search often misses the subtle semantic nuance.\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=fetch_k)\n",
    "    \n",
    "    # 2. Reranking: Use FlashRank (runs locally, no API key required)\n",
    "    # 'ms-marco-MiniLM-L-12-v2' is a small, fast model (~40MB download once).\n",
    "    reranker = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\")\n",
    "    \n",
    "    # This compresses the list: it ranks them and selects the top ones\n",
    "    compressed_docs = reranker.compress_documents(documents=retrieved_docs, query=query)\n",
    "    \n",
    "    # 3. Slice to keep only the absolute best 'k'\n",
    "    final_docs = compressed_docs[:k]\n",
    "\n",
    "    # 4. Serialize for the LLM\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata.get('source', 'Unknown')}\\nContent: {doc.page_content}\")\n",
    "        for doc in final_docs\n",
    "    )\n",
    "    return serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b79da7fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flashrank'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflashrank\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ranker, RerankRequest\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_context_with_rerank\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, vector_store, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, fetch_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Retrieves documents and reranks them using FlashRank directly (bypassing LangChain wrapper).\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flashrank'"
     ]
    }
   ],
   "source": [
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "def retrieve_context_with_rerank(query: str, vector_store, k=5, fetch_k=20):\n",
    "    \"\"\"\n",
    "    Retrieves documents and reranks them using FlashRank directly (bypassing LangChain wrapper).\n",
    "    \"\"\"\n",
    "    # 1. Initial Retrieval: Get a larger candidate set\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=fetch_k)\n",
    "    \n",
    "    # 2. Prepare data for FlashRank\n",
    "    # FlashRank expects a list of dictionaries with specific keys (like \"id\" and \"text\")\n",
    "    passages = []\n",
    "    for idx, doc in enumerate(retrieved_docs):\n",
    "        passages.append({\n",
    "            \"id\": idx,\n",
    "            \"text\": doc.page_content,\n",
    "            \"meta\": doc.metadata\n",
    "        })\n",
    "    \n",
    "    # 3. Reranking\n",
    "    # We initialize the Ranker locally. 'ms-marco-MiniLM-L-12-v2' is efficient and standard.\n",
    "    ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\", cache_dir=\"./opt\")\n",
    "    \n",
    "    # rank() returns the list sorted by relevance score\n",
    "    reranked_results = ranker.rank(query=query, docs=passages)\n",
    "    \n",
    "    # 4. Slice to keep only the top 'k'\n",
    "    top_results = reranked_results[:k]\n",
    "    \n",
    "    # 5. Serialize for the LLM\n",
    "    # We reconstruct the string from the reranked dictionary results\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {res['meta'].get('source', 'Unknown')}\\nContent: {res['text']}\")\n",
    "        for res in top_results\n",
    "    )\n",
    "    \n",
    "    return serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bb05ac8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the model once (it will download automatically)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 'ms-marco-MiniLM-L-6-v2' is a fast, high-quality reranker\u001b[39;00m\n\u001b[0;32m      5\u001b[0m reranker_model \u001b[38;5;241m=\u001b[39m CrossEncoder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcross-encoder/ms-marco-MiniLM-L-6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Load the model once (it will download automatically)\n",
    "# 'ms-marco-MiniLM-L-6-v2' is a fast, high-quality reranker\n",
    "reranker_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def retrieve_context_with_rerank(query: str, vector_store, k=5, fetch_k=20):\n",
    "    \"\"\"\n",
    "    Retrieves documents and reranks them using Sentence-Transformers (CrossEncoder).\n",
    "    \"\"\"\n",
    "    # 1. Initial Retrieval: Get a larger candidate set\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=fetch_k)\n",
    "    \n",
    "    # 2. Prepare pairs for the CrossEncoder\n",
    "    # The model expects a list of [query, document_text] pairs\n",
    "    doc_texts = [doc.page_content for doc in retrieved_docs]\n",
    "    pairs = [[query, doc_text] for doc_text in doc_texts]\n",
    "    \n",
    "    # 3. Score the pairs\n",
    "    # This returns a list of float scores\n",
    "    scores = reranker_model.predict(pairs)\n",
    "    \n",
    "    # 4. Attach scores to docs and sort\n",
    "    # We zip them together to keep track of which score belongs to which doc\n",
    "    scored_docs = sorted(\n",
    "        zip(retrieved_docs, scores), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    # 5. Slice to keep only the top 'k'\n",
    "    top_docs = [doc for doc, score in scored_docs[:k]]\n",
    "    \n",
    "    # 6. Serialize for the LLM\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata.get('source', 'Unknown')}\\nContent: {doc.page_content}\")\n",
    "        for doc in top_docs\n",
    "    )\n",
    "    \n",
    "    return serialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "91e1c8f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "PydanticUserError",
     "evalue": "`FlashrankRerank` is not fully defined; you should define `Ranker`, then call `FlashrankRerank.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/class-not-fully-defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPydanticUserError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mretrieve_context_with_rerank\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is the purpose of this project?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_vector_store\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[50], line 19\u001b[0m, in \u001b[0;36mretrieve_context_with_rerank\u001b[1;34m(query, vector_store, k, fetch_k)\u001b[0m\n\u001b[0;32m     15\u001b[0m retrieved_docs \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39msimilarity_search(query, k\u001b[38;5;241m=\u001b[39mfetch_k)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 2. Reranking: Use FlashRank (runs locally, no API key required)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 'ms-marco-MiniLM-L-12-v2' is a small, fast model (~40MB download once).\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m reranker \u001b[38;5;241m=\u001b[39m \u001b[43mFlashrankRerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mms-marco-MiniLM-L-12-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# This compresses the list: it ranks them and selects the top ones\u001b[39;00m\n\u001b[0;32m     22\u001b[0m compressed_docs \u001b[38;5;241m=\u001b[39m reranker\u001b[38;5;241m.\u001b[39mcompress_documents(documents\u001b[38;5;241m=\u001b[39mretrieved_docs, query\u001b[38;5;241m=\u001b[39mquery)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\yotam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\_internal\\_mock_val_ser.py:99\u001b[0m, in \u001b[0;36mMockValSer.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# raise an AttributeError if `item` doesn't exist\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_val_or_ser, item)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_message, code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_code)\n",
      "\u001b[1;31mPydanticUserError\u001b[0m: `FlashrankRerank` is not fully defined; you should define `Ranker`, then call `FlashrankRerank.model_rebuild()`.\n\nFor further information visit https://errors.pydantic.dev/2.9/u/class-not-fully-defined"
     ]
    }
   ],
   "source": [
    "retrieve_context_with_rerank(\"What is the purpose of this project?\", loaded_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c98403",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f9a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrix = ['groundness', 'relevance', 'completeness', 'helpfulness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd30840",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answer_groundness ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc95b95e",
   "metadata": {},
   "source": [
    "things to add:\n",
    "- bm25\n",
    "- reranker\n",
    "\n",
    "desitions to make:\n",
    "- a chanking strategy - done (for now)\n",
    "- emmbeding model \n",
    "- k\n",
    "- prompt\n",
    "- tagging? retrive by context\n",
    "\n",
    "evaluation \n",
    "- chank rellevency\n",
    "- recall (the rellevent chank returned?)\n",
    "- answer groundness\n",
    "- answer correctnes\n",
    "- answer relleventness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104197fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embbeding_models_list = ['text-embedding-3-small','text-embedding-3-large','text-embedding-ada-002']\n",
    "chanking_strategy_list = ['recursive', 'markdown','hierarchical', 'simple_splitter']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
